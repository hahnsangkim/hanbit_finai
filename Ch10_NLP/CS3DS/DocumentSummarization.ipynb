{"nbformat":4,"nbformat_minor":0,"metadata":{"_change_revision":206,"_is_fork":false,"kernelspec":{"name":"python3","display_name":"Python 3.8.5 64-bit ('hanbit_finai-xEqNmjSS')","metadata":{"interpreter":{"hash":"f5fc7fda28e31e2cabe125312e65ea385ea915cc0c6d7a8b10f6c868700ae883"}}},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5-final"},"colab":{"name":"DocumentSummarization.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"83708667-4fdc-1563-7b3a-06b6575d2865","id":"V3i-o3GKy2Sg"},"source":["# Document Summarization\n","\n","In this case study we use LDA model to classify documents into topics and sub-topics. "]},{"cell_type":"markdown","metadata":{"id":"u-4R4lSGy2Sl"},"source":["## Content"]},{"cell_type":"markdown","metadata":{"id":"5x-sXuSYy2Sl"},"source":["* [1. Problem Definition](#0)\n","* [2. Getting Started - Load Libraries and Dataset](#1)\n","    * [2.1. Load Libraries](#1.1) \n","* [3. Data Preparation](#2)\n","* [4.Model Construction and Training](#4)        \n","* [5.Visualization of results](#5) \n","    * [5.1. Topic Visualization](#5.1) \n","    * [5.2. Word Cloud](#5.2) "]},{"cell_type":"markdown","metadata":{"id":"EOYv8n7uy2Sm"},"source":["<a id='0'></a>\n","# 1. Problem Definition"]},{"cell_type":"markdown","metadata":{"id":"Z3zFtgZty2Sm"},"source":["The goal of this project is to effectively discover common\n","topics among a large data set of earnings call transcripts\n","of publicly traded companies. Each transcript will be assigned to some number of topics, and the specific segments\n","of the transcript which address a given topic will hopefully\n","be specified as well. Thus, not only will the documents be\n","classified as covering some set of topics, but the documents\n","themselves will be partitioned into different sub-topics. \n"]},{"cell_type":"markdown","metadata":{"id":"qclpyI4-y2Sm"},"source":["<a id='1'></a>\n","# 2. Getting Started- Loading the data and python packages\n"]},{"cell_type":"markdown","metadata":{"id":"mZ7IXa9ny2Sm"},"source":["<a id='1.1'></a>\n","## 2.1. Loading the python packages\n","\n","In the first step we check if the additional packages needed are present, if not install them. These are checked separately as they aren't included in requirement.txt as they aren't used for all case studies."]},{"cell_type":"code","metadata":{"id":"41dRyK5Oy2Sn"},"source":["import pkg_resources\n","# import pip\n","installedPackages = {pkg.key for pkg in pkg_resources.working_set}\n","required = {'pdfminer', 'pyldavis', 'wordcloud','mglearn'}\n","if 'pdfminer' not in installedPackages:\n","    !pip install -U pdfminer #==20191125\n","if 'pyldavis' not in installedPackages:\n","    !pip install -U pyLDAvis #==2.1.2\n","if 'wordcloud' not in installedPackages:\n","    !pip install -U wordcloud #==1.6.0\n","if 'mglearn' not in installedPackages:\n","    !pip install -U mglearn"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5hYfMV9gy2So"},"source":["All the libraries for feature extraction and topic modeling are loaded. The libraries for the vizualisation will be loaded later. "]},{"cell_type":"code","metadata":{"_cell_guid":"5d8fee34-f454-2642-8b06-ed719f0317e1","id":"Tn-NwMg7y2So"},"source":["#Libraries for pdf conversion\n","from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n","from pdfminer.converter import TextConverter\n","from pdfminer.layout import LAParams\n","from pdfminer.pdfpage import PDFPage\n","import re\n","from io import StringIO\n","\n","#Libraries for feature extraction and topic modeling\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.decomposition import LatentDirichletAllocation\n","\n","#Other libraries\n","import numpy as np\n","import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A4XywxBvy2So"},"source":["if you encounter the error, ModuleNotFoundError: No module named 'sklearn.feature_extraction.stop_words'\n","change it into 'from sklearn.feature_extraction._stop_words'\n"]},{"cell_type":"code","metadata":{"id":"ntYEKITBy2Sp"},"source":["#Diable the warnings\n","import warnings\n","warnings.filterwarnings('ignore')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V5jzPenWy2Sp"},"source":["<a id='2'></a>\n","# 3. Data Preparation"]},{"cell_type":"markdown","metadata":{"id":"gU-lCxICy2Sp"},"source":["The function defined below pulls out all characters from a pdf document except the images. The function simply takes in the name of the pdf document, extracts all characters from it and outputs the extracted texts as a python list of strings."]},{"cell_type":"code","metadata":{"id":"5Me_zhZCzLHP"},"source":["thisiscolab = False\n","working_dir = '../../data/'\n","if thisiscolab:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    root_dir = '/content/drive/MyDrive/Colab Notebooks/hanbit_mlfi/'\n","    root_dir = root_dir + 'Ch10_NLP/CS3DS'\n","    from os import chdir\n","    chdir(root_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cqfuDFXEy2Sp"},"source":["def convert_pdf_to_txt(path):\n","    rsrcmgr = PDFResourceManager()\n","    retstr = StringIO()\n","    laparams = LAParams()\n","    device = TextConverter(rsrcmgr, retstr, laparams=laparams)\n","    fp = open(path, 'rb')\n","    interpreter = PDFPageInterpreter(rsrcmgr, device)\n","    password = \"\"\n","    maxpages = 0\n","    caching = True\n","    pagenos=set()\n","\n","    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=True):\n","        interpreter.process_page(page)\n","\n","    text = retstr.getvalue()\n","\n","    fp.close()\n","    device.close()\n","    retstr.close()\n","    return text"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PZbFag1Qy2Sp"},"source":["The pdf is converted to text using the function defined above."]},{"cell_type":"code","metadata":{"id":"OcW3fY0Gy2Sq"},"source":["Document=convert_pdf_to_txt('10K.pdf')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8jecNOZJy2Sq"},"source":["f=open('Finance10k.txt','w')\n","f.write(Document)\n","f.close()\n","with open('Finance10k.txt') as f:\n","    clean_cont = f.read().splitlines()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wfw9D3say2Sq"},"source":["Let us look at the raw document"]},{"cell_type":"code","metadata":{"id":"jFAIreoky2Sq"},"source":["clean_cont[1:15]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bobKqKq1y2Sq"},"source":["The text extracted from the pdf document contains uninformative characters which needs to be removed. "]},{"cell_type":"code","metadata":{"id":"ADWyf3Z1y2Sr"},"source":["doc=[i.replace('\\xe2\\x80\\x9c','') for i in clean_cont ]\n","doc=[i.replace('\\xe2\\x80\\x9d','') for i in doc ]\n","doc=[i.replace('\\xe2\\x80\\x99s','') for i in doc ]\n","\n","docs = [x for x in doc if x != ' ']\n","docss = [x for x in docs if x != '']\n","financedoc=[re.sub(\"[^a-zA-Z]+\", \" \", s) for s in docss]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"On4fOeaPy2Sr"},"source":["financedoc[1:15]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M9T-6EpZy2Sr"},"source":["<a id='4'></a>\n","# 4. Model construction and training"]},{"cell_type":"markdown","metadata":{"id":"IljtPhf3y2Sr"},"source":["The scikit-learn module CountVectorizer was used with minimal parameter tuning to represent the clean document as a DocumentTermMatrix. This is because modeling requires that strings be represented as integers. The CountVectorizer shows the number of times a word occurs in the list after stop-words were removed.\n","The document term matrix was formatted into a pandas dataframe to glance the dataset, shown below. This dataframe shows count of word-occurrence of each term in the document. "]},{"cell_type":"code","metadata":{"id":"AspVhuxCy2Sr"},"source":["vect=CountVectorizer(ngram_range=(1,1),stop_words='english')\n","fin=vect.fit_transform(financedoc)\n","pd.DataFrame(fin.toarray(),columns=vect.get_feature_names()).head(1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jga9PEKEy2Ss"},"source":["This document term matrix was used as the input data to be used by the Latent Dirichlet Allocation algorithm for topic modeling. The algorithm was fitted to isolate five-distinct topic contexts as shown by the code below. This value can definitely be altered depending on the level of granularity one intends to obtain from the modeling."]},{"cell_type":"code","metadata":{"id":"qhrD5M8Ry2Ss"},"source":["lda=LatentDirichletAllocation(n_components=5)\n","lda.fit_transform(fin)\n","lda_dtf=lda.fit_transform(fin)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oc8HDM2_y2Ss"},"source":["sorting=np.argsort(lda.components_)[:,::-1]\n","features=np.array(vect.get_feature_names())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GYHf1S9Oy2Ss"},"source":["# The following is the code added so that the column names Topic names in mglearn start with 1, \n","# if this code is not added the topic names start with 0 and there is misalignment in the topic names \n","# from the output of pyLDAvis in the next section. \n","array=np.full((1, sorting.shape[1]), 1)\n","array = np.concatenate((array,sorting), axis=0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6ept5Zfny2Ss"},"source":["The code below uses the mglearn library to display the top 10 words within each specific topic model. One could easily draw conclusions what each topic summarizes from the words presented."]},{"cell_type":"code","metadata":{"id":"3WGILbKby2St"},"source":["import mglearn\n","topics = mglearn.tools.print_topics(topics=range(1,6), feature_names=features,\n","sorting=array, topics_per_chunk=5, n_words=10)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VZ6Qq7v6y2St"},"source":["Each topic in the table above is expected to represent a broader theme. However, given we trained the model on only on one document the theme within the topics may not be very distinct from each other. Topic-2 discusses quarters, months and  currency units related to asset valuation. Topic-3 talks about income from real estate, mortgage and related instrument. Topic-5 has terms related to asset valuation. Topic-1 has much to do with balance sheet items. Topic-4 is slighly similar to Topic-1 and has words related to instruments and markets. \n","\n","In terms of overall theme, topic 2 and topic 5 are quite distinct from others. There might be some similarity between topics 1 and 4 from the words we see under these topics. In the next section we will try to understand the separation between these topics using the python library pyLDAvis."]},{"cell_type":"markdown","metadata":{"id":"o4cbyXGVy2St"},"source":["<a id='5'></a>\n","# 5. Vizualisation of Topics"]},{"cell_type":"markdown","metadata":{"id":"HjSiqZbty2Su"},"source":["<a id='5.1'></a>\n","## 5.1 Topics Visualization\n","\n","Topic visualization facilitates the evaluation of topic quality using human judgment.\n","pyLDAvis is library that displays the global relationships between topics while also facilitating their\n","semantic evaluation by inspecting the terms most closely associated with each topic and,\n","inversely, the topics associated with each term. It also addresses the challenge that terms\n","that are frequent in a corpus tend to dominate the multinomial distribution over words that\n","define a topic. The PyldaVis library was used to visualize the topic models. "]},{"cell_type":"code","metadata":{"id":"t53a5fZmy2Su"},"source":["from __future__ import  print_function\n","import pyLDAvis\n","import pyLDAvis.sklearn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V9beHXDIy2Su"},"source":["zit=pyLDAvis.sklearn.prepare(lda,fin,vect)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pGxmVbymy2Su"},"source":["pyLDAvis.display(zit)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BVK9duU_y2Su"},"source":["The display used for the purpose of the book is shown below. "]},{"cell_type":"markdown","metadata":{"id":"Co5cbSfDy2Su"},"source":["![](topics2.png)"]},{"cell_type":"markdown","metadata":{"id":"aaL3M-y6y2Sv"},"source":["We notice that topic 2 and topic 5 are quite distant from each other. This is what we observed in the section above from the words and overall theme of these topics. However, topics 1 and 4 quite close as we observed before. These topics should be analyzed more intricately as they might be combined if needed to have clear separation of the topics. The relevance of the terms under each topic as shown in the right panel of the chart above can also be used to understand the differences. Topics 3 and 4 are relatively closer as well, although 3 is quite distant from other topics."]},{"cell_type":"markdown","metadata":{"id":"eJbLT7iKy2Sv"},"source":["<a id='5.2'></a>\n","## 5. WordCloud"]},{"cell_type":"markdown","metadata":{"id":"vcwTkhEmy2Sv"},"source":["A wordcloud was also generated for the entire legal document to note the most recurrent terms in the document as shown in the figure below. "]},{"cell_type":"code","metadata":{"id":"7yxJzuewy2Sv"},"source":["from os import path\n","from PIL import Image\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from wordcloud import WordCloud,STOPWORDS"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rk1iVPh_y2Sv"},"source":["d = path.dirname(__name__)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U4M3B7iuy2Sw"},"source":["text = open(path.join(d, 'Finance10k.txt')).read()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GjFpXkN9y2Sw"},"source":["#raw_pic = np.array(Image.open(path.join(d, \"legalpic.png\")))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J4pO5gZyy2Sw"},"source":["stopwords = set(STOPWORDS)\n","wc = WordCloud(background_color=\"black\", max_words=2000, stopwords=stopwords)\n","wc.generate(text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NQw0gLDny2Sw"},"source":["plt.figure(figsize=(16,13))\n","plt.imshow(wc, interpolation='bilinear')\n","plt.axis(\"off\")\n","plt.figure()\n","#plt.imshow(raw_pic, cmap=plt.cm.gray, interpolation='bilinear')\n","plt.axis(\"off\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4du2fq6Ky2Sw"},"source":["The word cloud generally agrees with the results from the topic modeling as words like : LOAN,REAL ESTATE, THIRD QUARTER,INTEREST RATE, CREDIT LOSSES etc are seen to be recurrent and hence bolder .\n","\n","By integrating Topics’s 1, 3 and 4 obtained by the Latent Dirichlet Allocation modeling with the Word Cloud generated for the finance document, we can safely deduce that this document is a simple Third Quarter Financial Balance sheet with all credit and assets values in that quarter with respect to all assets values."]},{"cell_type":"markdown","metadata":{"id":"Fq8soUrhy2Sx"},"source":["\n","**Conclusion**\n","\n","In this case study, we demonstrated the use of LDA model that extracts plausible\n","topics that allow us to gain a high-level understanding of large amounts of text in an\n","automated way, while also identifying relevant documents in a targeted way.\n","\n","Overall, the case study shows how machine learning and NLP can be applied across\n","many domains such as investment analysis, asset modeling, risk management, regula‐\n","tory and legal compliance to summarize documents, news and reports and signifi‐\n","cantly reduce the manual processing. \n","\n"]}]}